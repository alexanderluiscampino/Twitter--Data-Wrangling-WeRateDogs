{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Efforts\n",
    "\n",
    "During this project many data wrangling processes and efforts were made. Firstly, it was given a file with close to 2500 Tweets entries with several different information about each tweet. Opening this csv file and exporting it to a Pandas Dataframe was rather easy and straightforward.\n",
    "\n",
    "The most demanding exercise was:\n",
    "    - Using the Twitter API to query information from each tweet present on the CSV file given. \n",
    "    - Establishing the connection and querying the information was a process that took some time, but once established and set-up, worked exceptionlly well. \n",
    "\n",
    "Once the information was gathered, it was saved on a json .txt file. \n",
    "    - This file contained a json entry with many different fields from each individual tweet. \n",
    "    - Close to 2500 entries. This file resulted to be extremely long and extensive in information. \n",
    "    - The Tweet API return a json file from each tweet on which we performed a query. \n",
    "    - This Json information contains many fields, most of which were not used during this project.\n",
    "\n",
    "The major problem on data gathering was:\n",
    "     - The data of the json file created was organized into a comprehensive dataframe\n",
    "     - Information from each tweet ID present was gathered. \n",
    "     - Json returned from Tweet API was not very clear\n",
    "     - Determining which Tweets were a retweet, a reply, original, or self retwet was rather complicated.\n",
    "     - This information was not readily available, or the format would change from tweet to tweet\n",
    "     - Json format seems to have changed with time, which complicated the gathering process \n",
    "     - This meant some extra work investigating programmatically each tweet\n",
    "     - Each Tweet's text was analysed individually, looking for specific entries that determined the type of Tweet\n",
    "\n",
    "Once all this information was gathered, a dataframe was created. \n",
    "     - Dataframe was complemented with extra information\n",
    "     - From the CSV file it was merged the Dog Stage information\n",
    "     - From the TSV file the Breed prediction was gathered\n",
    "\n",
    "Cleaning the data is always the most demanding process. Efforts wre made in the following areas:\n",
    "     - Cleaning entries that do not represent a tweet about a dog\n",
    "     - Cleaning wrong ratings or missing ratings\n",
    "     - All tweets without a link were corrected\n",
    "     - Missing Dog's names were obtained, as much as possible.\n",
    "     - NaN values were removed or replaced by the correct ones\n",
    "     - Information coming from 3 different sources were merged in one comprehensive dataset\n",
    "     - From that unique dataset, derivatives datasets were created with important information\n",
    "\n",
    "The author attempted to clean the data as much as possible, all efforts made are extensively reproted on the project code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
